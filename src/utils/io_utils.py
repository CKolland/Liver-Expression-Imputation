import copy
from dataclasses import dataclass, field
import logging
from pathlib import Path
import sys
from typing import Any

from anndata import AnnData
import numpy as np
import pandas as pd
import tomli
from torch import float32, tensor, Tensor
from torch.utils.data import Dataset

import fit.constants as C


class ImputationDataset(Dataset):
    """A dataset used for data imputation using an annotated data matrix.

    A dataset class that inherits from `torch.utils.data.Dataset`.
    The Dataset takes an `AnnData` object as input. A list of column entries needs to be provided
    as an additional input to construct the model input. The whole matrix is then used as
    reconstruction label for the model.
    """

    def __init__(self, adata: AnnData, input_subset: list[str]):
        """Initialize the dataset.

        :param AnnData adata: The annotated data matrix
        :param list[str] input_subset: List of gene names that are used as input
        """
        self.adata = adata
        self.input_subset = input_subset

    def __len__(self) -> int:
        """Return the number of samples in the dataset.

        :return: Number of rows (cells) in the AnnData object
        :rtype: int
        """
        return self.adata.shape[0]

    def __getitem__(self, idx: int) -> tuple[Tensor, Tensor]:
        """Retrieve the input features and corresponding label for a given index.

        The `input` tensor is generated by selecting the specified features from the annotated data
        matrix, as defined by `input_subset`. The `label` tensor consists of the entire row for the
        given sample.

        :param int idx: Index of the sample to retrieve

        :return: A tuple containing the input tensor and the label tensor
        :rtype: tuple[Tensor, Tensor]

        :raises IndexError: If the provided index is out of range
        """
        if idx >= len(self):
            raise IndexError("Index out of range")

        # Select row of reduced features from AnnData object
        # `.toarray()` converts to dense array
        # `.squeeze()` converts to one-dimensional vector
        input = self.adata[:, self.input_subset].X.getrow(idx).toarray().squeeze()

        # Retrieve the corresponding annotation label
        label = self.adata.X.getrow(idx).toarray().squeeze()

        # Convert the row to a PyTorch tensor
        return tensor(input, dtype=float32), tensor(label, dtype=float32)


@dataclass
class TrainingMetrics:
    """Container for tracking training metrics across epochs and folds.

    This class stores per-epoch training and validation metrics for each fold,
    identifies the best-performing fold based on validation loss, and provides
    utility methods to export the collected metrics for further analysis or
    visualization.

    :param dict[int, dict[str, list[float]]] fold_metrics: Mapping of fold numbers to their associated metrics.
    :param Optional[int] best_fold: Index of the fold with the best validation performance.
    :param float best_val_loss: Lowest validation loss observed across all folds.
    :param Optional[dict[str, Any]] best_model_state: State dictionary of the best-performing model.
    """

    fold_metrics: dict[int, dict[str, list[float]]] = field(default_factory=dict)
    best_fold: int | None = None
    best_val_loss: float = float("inf")
    best_model_state: dict[str, Any] | None = None

    def add_fold_epoch(
        self,
        fold: int,
        epoch: int,
        train_loss: float,
        grad_norm: float,
        max_grad: float,
        val_loss: float,
    ):
        """Add metrics for a specific epoch and fold.

        Initializes a new fold entry if it does not exist, then appends the
        given metrics to the corresponding lists.

        :param int fold: Fold index (0-based).
        :param int epoch: Epoch number (0-based).
        :param float train_loss: Training loss for the current epoch.
        :param float grad_norm: L2 norm of the gradients.
        :param float max_grad: Maximum gradient value in the epoch.
        :param float val_loss: Validation loss for the current epoch.
        """
        if fold not in self.fold_metrics:
            self.fold_metrics[fold] = {
                "epochs": [],
                "train_loss": [],
                "grad_norm": [],
                "max_grad": [],
                "val_loss": [],
            }

        self.fold_metrics[fold]["epochs"].append(epoch)
        self.fold_metrics[fold]["train_loss"].append(train_loss)
        self.fold_metrics[fold]["grad_norm"].append(grad_norm)
        self.fold_metrics[fold]["max_grad"].append(max_grad)
        self.fold_metrics[fold]["val_loss"].append(val_loss)

    def update_best_model(
        self,
        fold: int,
        val_loss: float,
        model_state: dict[str, Any],
    ):
        """Update the best model if the current validation loss is lower.

        Stores the model state and updates fold metadata if the new validation
        loss is better than any previously recorded value.

        :param int fold: Fold index of the current model.
        :param float val_loss: Validation loss to compare against the best.
        :param dict[str, Any] model_state: State dictionary of the model.
        """
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.best_fold = fold
            self.best_model_state = copy.deepcopy(model_state)

    def to_data_frame(self) -> pd.DataFrame:
        """Convert collected metrics to a long-format DataFrame.

        The resulting DataFrame includes fold, epoch, training loss, gradient norm
        max gradient norm and validation loss for each training epoch.

        :return: DataFrame containing per-epoch training and validation metrics.
        :rtype: pd.DataFrame
        """
        results = []

        for fold, metrics in self.fold_metrics.items():
            for i, epoch in enumerate(metrics["epochs"]):
                results.append(
                    {
                        "fold": fold,
                        "epoch": epoch,
                        "train_loss": metrics["train_loss"][i],
                        "grad_norm": metrics["grad_norm"][i],
                        "max_grad": metrics["max_grad"][i],
                        "val_loss": metrics["val_loss"][i],
                    }
                )

        return pd.DataFrame(results)

    def get_fold_summary(self) -> pd.DataFrame:
        """Summarize metrics per fold.

        Provides a summary DataFrame containing the final training loss,
        final validation loss, minimum validation loss, and number of epochs
        trained for each fold.

        :return: DataFrame with one row per fold summarizing training outcomes.
        :rtype: pd.DataFrame
        """
        summary = []

        for fold, metrics in self.fold_metrics.items():
            summary.append(
                {
                    "fold": fold,
                    "final_train_loss": (
                        metrics["train_loss"][-1] if metrics["train_loss"] else np.nan
                    ),
                    "final_val_loss": (
                        metrics["val_loss"][-1] if metrics["val_loss"] else np.nan
                    ),
                    "min_val_loss": (
                        min(metrics["val_loss"]) if metrics["val_loss"] else np.nan
                    ),
                    "epochs_trained": len(metrics["epochs"]),
                }
            )

        return pd.DataFrame(summary)


def assert_path(path: str, assert_dir: bool = True) -> Path:
    """Validates that a given path exists and is either a file or a directory.

    Converts the input string to a ``Path`` object and checks whether it exists and matches
    the expected type (file or directory), based on the ``assert_dir`` flag.

    :param str path: The file system path to validate.
    :param bool assert_dir: If ``True``, asserts that the path is a directory.
                            If ``False``, asserts that the path is a file.

    :raises TypeError: If ``path`` is not a string.
    :raises TypeError: If ``assert_dir`` is not a boolean.
    :raises FileNotFoundError: If the path does not exist.
    :raises NotADirectoryError: If ``assert_dir=True`` but the path is a file.
    :raises IsADirectoryError: If ``assert_dir=False`` but the path is a directory.

    :return: The validated path as a ``Path`` object.
    :rtype: pathlib.Path
    """
    assert isinstance(path, str), "Path input must be a string."
    assert isinstance(
        assert_dir, bool
    ), "Expected 'True' or 'False' to indicate whether a directory or file is asserted."

    # Convert path string to `Path`
    path = Path(path)

    if assert_dir:
        if path.is_dir() and path.exists():
            return path
        elif path.is_file():
            raise NotADirectoryError(f"Expected a directory, but `{path}` is a file.")
        else:
            raise FileNotFoundError(f"Directory `{path}` does not exist.")
    else:
        if path.is_file() and path.exists():
            return path
        elif path.is_dir():
            raise IsADirectoryError(f"Expected a file, but `{path}` is a directory.")
        else:
            raise FileNotFoundError(f"File `{path}` does not exist.")


def load_toml(toml_path: Path) -> dict:
    """Load and parse a TOML configuration file.

    This function reads and parses a TOML file using the `tomli` library.

    :param Path toml_path: Verified path to the TOML file.

    :return: Parsed TOML content as a dictionary.
    :rtype: dict
    """
    # Load TOML config
    with open(toml_path, "rb") as f:
        return tomli.load(f)


def setup_logging(path_to_log: str, logger_name: str) -> logging.Logger:
    """Set up and configure logging to both console and file.

    :param str path_to_log: Path to the log file where logs will be written.
    :param str logger_name: Name of the logger.

    :return: Configured logger instance.
    :rtype: logging.Logger
    """

    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.DEBUG)  # Lowest log level (logs everything)

    # Custom formatter
    formatter = logging.Formatter(C.LOGGING_FORMAT, datefmt=C.LOGGING_DATEFMT)

    # Add console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)  # Set handler specific log level
    console_handler.setFormatter(formatter)  # Add custom formatter to handler
    logger.addHandler(console_handler)  # Add handler to the logger

    # Add file handler
    file_handler = logging.FileHandler(path_to_log)
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger
