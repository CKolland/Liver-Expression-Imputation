import copy
from dataclasses import dataclass, field
import logging
from pathlib import Path
import sys
from typing import Any, Optional


from anndata import AnnData
import numpy as np
import pandas as pd
import tomli
from torch import float32, tensor, Tensor
from torch.utils.data import Dataset


class ImputationDataset(Dataset):
    """A dataset used for data imputation using an annotated data matrix.

    A dataset class that inherits from `torch.utils.data.Dataset`.
    The Dataset takes an `AnnData` object as input. A list of column entries needs to be provided
    as an additional input to construct the model input. The whole matrix is then used as
    reconstruction label for the model.
    """

    def __init__(self, adata: AnnData, input_subset: list[str]):
        """Initialize the dataset.

        :param AnnData adata: The annotated data matrix
        :param list[str] input_subset: List of gene names that are used as input
        """
        self.adata = adata
        self.input_subset = input_subset

    def __len__(self) -> int:
        """Return the number of samples in the dataset.

        :return: Number of rows (cells) in the AnnData object
        :rtype: int
        """
        return self.adata.shape[0]

    def __getitem__(self, idx: int) -> tuple[Tensor, Tensor]:
        """Retrieve the input features and corresponding label for a given index.

        The `input` tensor is generated by selecting the specified features from the annotated data
        matrix, as defined by `input_subset`. The `label` tensor consists of the entire row for the
        given sample.

        :param int idx: Index of the sample to retrieve

        :return: A tuple containing the input tensor and the label tensor
        :rtype: tuple[Tensor, Tensor]

        :raises IndexError: If the provided index is out of range
        """
        if idx >= len(self):
            raise IndexError("Index out of range")

        # Select row of reduced features from AnnData object
        # `.toarray()` converts to dense array
        # `.squeeze()` converts to one-dimensional vector
        input = self.adata[:, self.input_subset].X.getrow(idx).toarray().squeeze()

        # Retrieve the corresponding annotation label
        label = self.adata.X.getrow(idx).toarray().squeeze()

        # Convert the row to a PyTorch tensor
        return tensor(input, dtype=float32), tensor(label, dtype=float32)


@dataclass
class TrainingMetrics:
    """Container for tracking training metrics across epochs and folds."""

    fold_metrics: dict[int, dict[str, list[float]]] = field(default_factory=dict)
    best_fold: Optional[int] = None
    best_val_loss: float = float("inf")
    best_model_state: Optional[dict[str, Any]] = None

    def add_fold_epoch(self, fold: int, epoch: int, train_loss: float, val_loss: float):
        """Add metrics for a specific fold and epoch."""
        if fold not in self.fold_metrics:
            self.fold_metrics[fold] = {"epochs": [], "train_loss": [], "val_loss": []}

        self.fold_metrics[fold]["epochs"].append(epoch)
        self.fold_metrics[fold]["train_loss"].append(train_loss)
        self.fold_metrics[fold]["val_loss"].append(val_loss)

    def update_best_model(
        self, fold: int, val_loss: float, model_state: dict[str, Any]
    ):
        """Update best model if current validation loss is better."""
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.best_fold = fold
            self.best_model_state = copy.deepcopy(model_state)

    def to_dataframe(self) -> pd.DataFrame:
        """Convert metrics to a comprehensive DataFrame."""
        all_data = []
        for fold, metrics in self.fold_metrics.items():
            for i, epoch in enumerate(metrics["epochs"]):
                all_data.append(
                    {
                        "fold": fold,
                        "epoch": epoch,
                        "train_loss": metrics["train_loss"][i],
                        "val_loss": metrics["val_loss"][i],
                    }
                )
        return pd.DataFrame(all_data)

    def get_fold_summary(self) -> pd.DataFrame:
        """Get summary statistics per fold."""
        summary_data = []
        for fold, metrics in self.fold_metrics.items():
            summary_data.append(
                {
                    "fold": fold,
                    "final_train_loss": (
                        metrics["train_loss"][-1] if metrics["train_loss"] else np.nan
                    ),
                    "final_val_loss": (
                        metrics["val_loss"][-1] if metrics["val_loss"] else np.nan
                    ),
                    "min_val_loss": (
                        min(metrics["val_loss"]) if metrics["val_loss"] else np.nan
                    ),
                    "epochs_trained": len(metrics["epochs"]),
                }
            )
        return pd.DataFrame(summary_data)


def assert_path(path: str, assert_dir: bool = True) -> Path:
    """Validates that a given path exists and is either a file or a directory.

    Converts the input string to a ``Path`` object and checks whether it exists and matches
    the expected type (file or directory), based on the ``assert_dir`` flag.

    :param str path: The file system path to validate.
    :param bool assert_dir: If ``True``, asserts that the path is a directory.
                            If ``False``, asserts that the path is a file.

    :raises TypeError: If ``path`` is not a string.
    :raises TypeError: If ``assert_dir`` is not a boolean.
    :raises FileNotFoundError: If the path does not exist.
    :raises NotADirectoryError: If ``assert_dir=True`` but the path is a file.
    :raises IsADirectoryError: If ``assert_dir=False`` but the path is a directory.

    :return: The validated path as a ``Path`` object.
    :rtype: pathlib.Path
    """
    assert isinstance(path, str), "Path input must be a string."
    assert isinstance(
        assert_dir, bool
    ), "Expected 'True' or 'False' to indicate whether a directory or file is asserted."

    # Convert path string to `Path`
    path = Path(path)

    if assert_dir:
        if path.is_dir() and path.exists():
            return path
        elif path.is_file():
            raise NotADirectoryError(f"Expected a directory, but `{path}` is a file.")
        else:
            raise FileNotFoundError(f"Directory `{path}` does not exist.")
    else:
        if path.is_file() and path.exists():
            return path
        elif path.is_dir():
            raise IsADirectoryError(f"Expected a file, but `{path}` is a directory.")
        else:
            raise FileNotFoundError(f"File `{path}` does not exist.")


def load_toml(toml_path: Path) -> dict:
    """Load and parse a TOML configuration file.

    This function reads and parses a TOML file using the `tomli` library.

    :param Path toml_path: Verified path to the TOML file.

    :return: Parsed TOML content as a dictionary.
    :rtype: dict
    """
    # Load TOML config
    with open(toml_path, "rb") as f:
        return tomli.load(f)


def setup_logging(path_to_log: str, logger_name: str) -> logging.Logger:
    """Set up and configure logging to both console and file.

    :param str path_to_log: Path to the log file where logs will be written.
    :param str logger_name: Name of the logger.

    :return: Configured logger instance.
    :rtype: logging.Logger
    """

    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.DEBUG)  # Lowest log level (logs everything)

    # Custom formatter
    formatter = logging.Formatter(
        "%(asctime)s || LEVEL: %(levelname)s |> %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Add console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)  # Set handler specific log level
    console_handler.setFormatter(formatter)  # Add custom formatter to handler
    logger.addHandler(console_handler)  # Add handler to the logger

    # Add file handler
    file_handler = logging.FileHandler(path_to_log)
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger
